services:
  spark-master:
    build: .
    container_name: spark-master
    hostname: spark-master
    environment:
      - SPARK_DAEMON_JAVA_OPTS=-Dlog4j2.disable.jmx=true
    ports:
      - "9090:8080"
      - "7077:7077"
    networks:
      - spark-network
    volumes:
      - ./jobs:/jobs
      - data_volume:/data

  spark-worker-1:
    build: .
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      - spark-master
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    ports:
      - "8081:8081"
    networks:
      - spark-network

  spark-worker-2:
    build: .
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      - spark-master
    environment:
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    ports:
      - "8082:8081"
    networks:
      - spark-network

  zookeeper:
    image: bitnami/zookeeper:latest
    container_name: zookeeper
    hostname: zookeeper
    restart: always
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOO_JUTE_MAXBUFFER: 4194304
      JVMFLAGS=-Djute.maxbuffer: 4194304
      ALLOW_ANONYMOUS_LOGIN: "yes"
      ZOO_JAVA_OPTS: "-Djute.maxbuffer=4194304"
      4LW_COMMANDS_WHITELIST: srvr,mntr,cons
      ZOO_4LW_COMMANDS_WHITELIST: srvr,mntr,cons
      ZOO_SERVER_JVMFLAGS: "-Djute.maxbuffer=4194304"
    ports:
      - "23181:2181"
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    networks:
      - spark-network

  kafka:
    image: bitnami/kafka:latest
    container_name: kafka
    hostname: kafka
    restart: always
    depends_on:
      zookeeper:
        condition: service_started
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_LISTENERS: "INTERNAL://:9092,EXTERNAL://:39092"
      KAFKA_ADVERTISED_LISTENERS: "INTERNAL://kafka:9092,EXTERNAL://host.docker.internal:39092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      KAFKA_MESSAGE_MAX_BYTES: 4194304
      KAFKA_REPLICA_FETCH_MAX_BYTES: 4194304
      KAFKA_PRODUCER_MAX_REQUEST_SIZE: 4194304
    ports:
      - "9093:9092"
    volumes:
      - kafka_data:/var/lib/kafka
    networks:
      - spark-network

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    restart: always
    ports:
      - "8185:8080"
    depends_on:
      kafka:
        condition: service_started
      zookeeper:
        condition: service_started
    environment:
      - KAFKA_CLUSTERS_0_NAME=local-cluster
      - KAFKA_CLUSTERS_0_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181
      - KAFKA_CLUSTERS_0_SECURITY_PROTOCOL=PLAINTEXT
      - DYNAMIC_CONFIG_ENABLED=true

  airflow-webserver:
    build:
      context: .
      dockerfile: airflow/Dockerfile
    image: custom-airflow:latest
    container_name: airflow-webserver
    restart: always
    depends_on:
      - airflow-scheduler
      - airflow-postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - PYTHONWARNINGS=ignore::DeprecationWarning
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
      - ./jobs:/jobs
      - ./airflow/scripts:/opt/airflow/scripts
    ports:
      - "8088:8080"
    networks:
      - spark-network
    command: webserver

  airflow-scheduler:
    build: 
      context: .
      dockerfile: airflow/Dockerfile
    image: custom-airflow:latest
    container_name: airflow-scheduler
    restart: always
    depends_on:
      - airflow-postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      - PYTHONWARNINGS=ignore::DeprecationWarning
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
      - ./jobs:/jobs
      - ./airflow/scripts:/opt/airflow/scripts
    networks:
      - spark-network
    command: scheduler

  airflow-postgres:
    image: postgres:13
    container_name: airflow-postgres
    restart: always
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - spark-network

  airflow-init:
    build:
      context: .
      dockerfile: airflow/Dockerfile
    image: custom-airflow:latest
    container_name: airflow-init
    entrypoint: >
      bash -c "
        python /opt/airflow/scripts/generate_dag_config.py &&
        airflow db init &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin  --email akhilbansal0000@gmail.com"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
      - PYTHONWARNINGS=ignore::DeprecationWarning
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - /var/run/docker.sock:/var/run/docker.sock
      - ./jobs:/jobs
      - ./airflow/scripts:/opt/airflow/scripts
    depends_on:
      - airflow-postgres
    networks:
      - spark-network

networks:
  spark-network:
    driver: bridge

volumes:
  data_volume:
  zookeeper_data:
  zookeeper_logs:
  kafka_data:
  postgres_data: